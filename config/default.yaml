seed : 42

data : 
  train_batch_size : 16
  val_batch_size : 16
  test_batch_size : 16
  tokenizer_max_len : 128

train :
  num_train_epoch : 3
  weight_decay : 0.01
  learning_rate : 5e-5

  # modules/schedulers.py
  lr_scheduler : stepLR  #[stepLR]

  # modules/optimizers.py
  optimizer : AdamW  #[AdamW, Adam, SGD]

  # modules/losses.py
  loss : cross_entropy  #[cross_entropy, focal_loss]

  # modules/datasets.py
  dataset_type : ainize  #[default, punct, type_entity, ainize] 

model :
  # models/utils.py
  model_name : ainize/klue-bert-base-re 
  #[
  # TAEMIN_TOKEN_ATTENTION_RoBERTa, 
  # TAEMIN_TOKEN_ATTENTION_BERT,
  # TAEMIN_RoBERTa_LSTM,
  # TAEMIN_R_RoBERTa,
  # TAEMIN_CUSTOM_RBERT,
  # RBERT,
  # 그 외 허깅페이스 모델들
  #]

wandb :
  project_name : XXXX
  name : klue-bert-sweep
  method: grid
  parameters : 
    epochs : 
      values : 
      - 3
      - 4
      - 5
      - 10
    batch_size:
      values: 
      - 8
      - 16
      - 32
    learning_rate:
      values: 
      - 0.00002 
      - 0.00003 
      - 0.00005
    weight_decay:
      values: 
      - 0.0
      - 0.01
      - 0.1

path:
<<<<<<< HEAD
  # model : /opt/ml/level2_klue-nlp-08/checkpoint/klue_bert-base/2023-05-11 22.09.58/epoch=1-val_micro_f1=62.60.ckpt
  model : /opt/ml/level2_klue-nlp-08/checkpoint/klue_bert-base/2023-05-15 01.58.21/epoch=2-val_micro_f1=62.21.ckpt
  # train : ./dataset/train/new_train.csv
  train : ./dataset/train/other_entity_train.csv
=======
  model : /opt/ml/level2_klue-nlp-08/checkpoint/ainize_klue-bert-base-re/2023-05-15 11.12.37/epoch=2-val_micro_f1=90.60.ckpt
  train : ./dataset/train/new_train.csv
>>>>>>> main
  dev : ./dataset/train/new_dev.csv
  test : ./dataset/test/test_data.csv
  confusion_matrix : ./confusion_matrix/klue-bert-base
  
