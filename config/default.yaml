seed : 42

data : 
  train_batch_size : 16
  val_batch_size : 16
  test_batch_size : 16
  tokenizer_max_len : 256

train :
  num_train_epoch : 3
  weight_decay : 0.01
  learning_rate : 5e-5

  # modules/schedulers.py
  lr_scheduler : stepLR  #[stepLR]

  # modules/optimizers.py
  optimizer : AdamW  #[AdamW, Adam, SGD]

  # modules/losses.py
  loss : cross_entropy  #[cross_entropy, focal_loss]

  # modules/datasets.py
  dataset_type : default  #[default, punct, type_entity] 

model :
  # models/utils.py
  model_name : klue/bert-base  
  #[
  # TAEMIN_TOKEN_ATTENTION_RoBERTa, 
  # TAEMIN_TOKEN_ATTENTION_BERT,
  # TAEMIN_RoBERTa_LSTM,
  # TAEMIN_R_RoBERTa,
  # TAEMIN_CUSTOM_RBERT,
  # RBERT,
  # 그 외 허깅페이스 모델들
  #]

wandb :
  project_name : XXXX
  name : klue-bert-sweep
  method: grid
  parameters : 
    epochs : 
      values : 
      - 3
      - 4
      - 5
      - 10
    batch_size:
      values: 
      - 8
      - 16
      - 32
    learning_rate:
      values: 
      - 0.00002 
      - 0.00003 
      - 0.00005
    weight_decay:
      values: 
      - 0.0
      - 0.01
      - 0.1

path:
  model : /opt/ml/level2_klue-nlp-08/model/klue/bert-base/2023-05-08 14:47:42.317246/epoch=2-step=4872.ckpt
  train : ./dataset/train/new_train.csv
  dev : ./dataset/train/new_dev.csv
  test : ./dataset/test/test_data.csv
  confusion_matrix : ./confusion_matrix/klue-bert-base
  
