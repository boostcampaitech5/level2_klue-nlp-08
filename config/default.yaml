seed : 42
data :
  train_batch_size : 16
  val_batch_size : 16
  test_batch_size : 16
  tokenizer_max_len : 256
train :
  num_train_epoch : 7
  weight_decay : 0.0001
  learning_rate : 2e-5
  # modules/schedulers.py
  lr_scheduler : stepLR  #[stepLR]
  # modules/optimizers.py
  optimizer : Adabelief  #[AdamW, Adam, SGD]
  # modules/losses.py
  loss : cross_entropy  #[cross_entropy, focal_loss]
  # modules/datasets.py
  dataset_type : punct  #[default, punct, type_entity, ainize]
model :
  # models/utils.py
  model_name : SangwonYoon/klue-roberta-large-tapt
  #[
  # TAEMIN_TOKEN_ATTENTION_RoBERTa,
  # TAEMIN_TOKEN_ATTENTION_BERT,
  # TAEMIN_RoBERTa_LSTM,
  # TAEMIN_R_RoBERTa,
  # TAEMIN_CUSTOM_RBERT,
  # RBERT,
  # 그 외 허깅페이스 모델들
  #]
wandb :
  project_name : XXXX
  name : klue-bert-sweep
  method: grid
  parameters :
    epochs :
      values :
      - 3
      - 4
      - 5
      - 10
    batch_size:
      values:
      - 8
      - 16
      - 32
    learning_rate:
      values:
      - 0.00002
      - 0.00003
      - 0.00005
    weight_decay:
      values:
      - 0.0
      - 0.01
      - 0.1
path:
  model : /opt/ml/level2_klue-nlp-08/checkpoint/SangwonYoon_klue-roberta-large-tapt/2023-05-17 22.57.54/epoch=3-val_micro_f1=72.63.ckpt
  train : ./dataset/train/final_data.csv
  dev : ./dataset/train/forbidden_val.csv
  test : ./dataset/test/test_data.csv
  confusion_matrix : ./confusion_matrix/klue/roberta-large