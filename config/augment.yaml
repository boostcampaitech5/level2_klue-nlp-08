model_name : 
  klue/roberta-large

model_path : 
  ./augmentation/checkpoint-4000

tokenizers : 
  return_tensors : pt
  padding: True
  truncation: True
  maxlength: 256
  add_special_tokens: True

training_arguments : 
  output_dir: ./augmentation/augment
  overwrite_output_dir: True
  learning_rate: 0.00001
  num_train_epochs: 2
  per_gpu_train_batch_size: 16
  save_steps: 1000
  save_total_limit: 2
  logging_steps: 100
  weight_decay: 0.01

DataCollatorForLanguageModeling:
  mlm: True
  mlm_probability: 0.15
    
data_path : 
  test_path : ./dataset/test/test_data.csv 
  train_path : ./dataset/train/train.csv
  pretrain_path : ./augmentation/augmentation_model